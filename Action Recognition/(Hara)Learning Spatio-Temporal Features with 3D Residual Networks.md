- 在这篇文章中，提出了一个基于ResNets的3D CNNs以更好的动作表示。

- 主要贡献是探索3DResNet的效果。

- 对数据集的说明
  - HMDB51 和 UCF101 都是小型数据集，使用这些数据库很难训练出好的模型而不进行过拟合。
  - Sport-1M和YouTube-8这样的数据集足够大，但它们的标注不明确，而且只是视频级别的标签（比如包括与目标活动不相关的帧）。
  - Kinetics 数据集包括30万或超过400个类别的精简视频
  - ActivityNet数据集提供了来自200个人类动作类的示例，每个类平均有137个未修剪的视频，每个视频有1.41个活动实例
  - 使用Kinetics来优化3DResNets
- 网络框架
  - 网络和原始的ResNets的区别在于卷积核和池化层的维数，使用3D卷积和3D池化
  - 卷积核的大小为$3\times3\times3$,conv1的时间维度stride为1，类似于C3D
  - 输入片段的size为$3\times16\times112\times112$ ,输入16帧的片段
  - 在第二层开始进行一次最大池化。
  - 第三层到第五层的第一个卷积block实行下采样（时间和空间一起）
  - 最后是池化和维度为400的全连接层（类别数量）
  - 时间维度下采样了4次，最后时间维度只有1；空间维度下采样了5次
  - 使用resnet中的identity shortcuts，0填充。
  - <img src="https://raw.githubusercontent.com/liuzhaoo/markdown_pics/master/img/3dresnet.png" style="zoom:80%;" />

- 训练
  - 使用动量随机梯度下降，动量为0.9，权重衰减为0.001
  - 随机生成训练样本来进行数据扩充：先通过均匀抽样的方法选择每个样本的时间定位，然在在这些定位点生成16帧片段，如果视频小于16帧，则对其进行循环。然后从4个角或1个中心随机选择空间位置（裁剪）。除了位置之外，我们还选择每个样本的空间尺度来进行多尺度裁剪。尺度从以下进行选择$\{1,\frac{1}{2^{1/4}},\frac{1}{2^{1/2}},\frac{1}{2^{3/4}},\frac{1}{2} \}$ .1表示最大尺度(即尺寸为帧短边的长度)。裁剪的帧的纵横比为1。生成的样本水平翻转的概率为50%
  - 还对每个样本执行均值减法
  - 所有生成的样本都有与原始视频相同的类别标签
  - 4GPU,batchsize=256
  - 初始学习率设置为0.1，在验证损失饱和后，将其除以10三次

- 识别

  - 使用训练模型识别视频中的动作。采用滑动窗口的方式来生成输入片段(即每个视频被分成16个不重叠的帧片段。)每个片段进行最大尺度的中心裁剪。
  - 使用训练的模型估计每个片段的动作类别概率，再对视频中所有片段的概率求平均来识别动作。

- 试验阶段

  - 使用ActivityNet 和 Kinetics 数据集。视频总长度为849小时，活动实例总数为28,108个。数据集被随机分成三个不同的子集:训练、验证和测试，其中50%用于训练，25%用于验证和测试。

  - 首先在ActivityNet 上训练18层的网络并在Sports-1M上训练了C3D。这个实验的目的是探索在相对较小的数据集上训练3D ResNets。算法的精度不是基于完整的视频，而是基于16帧的片段。结果表明ActivityNet数据集太小，无法从头训练3D ResNets。如图所示，resnet3d发生了过拟合且性能不太好。

      <img src="https://raw.githubusercontent.com/liuzhaoo/markdown_pics/master/img/3d18.png" style="zoom:70%;" />

  - 然后在Kinetics 上训练34层网络，得到较好的效果
  - 使用大量的gpu并增加批处理大小、空间分辨率和时间持续时间可能会进一步改进3D ResNets。
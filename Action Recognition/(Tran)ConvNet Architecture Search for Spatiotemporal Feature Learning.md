### 摘要

 提出了深度3D残差网络，性能比C3D网络好得多：比C3D快两倍且小两倍。表示更加紧凑

### 引言

改进ConvNet架构的设计已经刺激了图像理解方面的重大进展：从AlexNet到VGG，然后是GoogleNet，最后是ResNet。视频理解是计算机视觉的另一个基础的问题，但视频分类架构[14,24,33]和表示学习[41]的进展较慢。阻碍强大结构的因素主要有三个：第一，与图像模型相比，视频的计算量和内存消耗更高。比如在C3D模型中，在UCF101上训练模型需要3到4天，在Sports-1M上需要两个月，因此导致在UCF101上寻找广泛的结构很难。第二，目前还没有一个用于视频架构搜索的标准基准。在静态的图像中，可以在一个合理的时间内在ImageNet上训练网络，而且在ImageNet上表现良好的结构已经被证明可以用于其他任务，比如对象检测和分割。在视频领域，Sport-1M被证明在通用特征学习上是有用的，但是用它来进行寻找结构还是太大了。相比之下，虽然UCF101和ImageNet有相似的帧数，但是它们是高度相关的，并且设置被严格控制。因此，在这个基准上训练的模型很容易过拟合，[41,14]的实验表明，从头训练的ConvNets可以获得41 - 44%的精度，而在UCF101上对Sports1M惊醒微调可以将精度提高到82%。第三，设计视频分类模型并非易事，有许多选择产生的结果是敏感的，其中包括如何对输入进行采样和预处理、卷积的类型、使用多少层以及如何对时间维度建模。因此，虽然图像领域的进展显然应该纳入视频建模，单纯地将图像模型转换为视频分类(例如简单地将2D Resnet应用于视频帧)是次优的。

在本文中，我们通过在一个小型基准上(UCF101)寻找一个精心设计的架构搜索来解决这些问题，有人可能会说，这些发现的普遍性受到了数据集的偏差的限制，会将搜索的结果过拟合到UCF101上。我们通过两项努力来解决这个问题。首先，我们限制这些网络使它们有相似的容量（参数的数量），他们仍然会过拟合，但是精度的提高更多的归功于结构的单一变化而不是容量。其次，在这个小数据集架构的搜索上使我们得到一个有效的三维深度残差网络，当我们在更大的数据集上(sport - 1m)进行训练时，证明了它的有效性，而且在不同的视频基准测试中都取得了不俗的成绩。综上所述，本文的贡献如下:

- 通过在UCF101上进行行为识别的训练来进行跨多个维度的ConvNet架构搜索，并提出对每个维度敏感性的经验观察

- 我们提出(据我们所知)第一个深度3D残差网络并在大规模视频基准上训练，用于时空特征学习。
- 我们的时空特征在Sport-1M（不使用长时间模型），UCF101和HMDB51（只考虑RGB输入）上，以及在THUMOS14和ASLAN竞赛的表现上都取得了sota性能。
- 我们的模型比目前的深度视频特征快2倍，小2倍，更紧凑。

### 2. 相关工作

视频理解是计算机视觉的核心问题之一，已经研究了几十年。许多对视频理解的研究都集中在发展视频的时空特征上。一些提出的视频表征包括时空兴趣点STIPs，SIFT-3D，HOG3D，Cuboids和ActionBank。这些表示是手工设计的，使用不同的特征编码方案，如特征直方图或金字塔在手工制作的表示中，改进密集轨迹(iDT)被认为是目前最先进的手工制作特征，在不同的视频分类问题上有很好的结果。

自计算机视觉深度学习在2012年ImageNet挑战赛上亮相取得突破以来，许多基于卷积神经网络的方法被提出来用于图像识别。Simonyan和Zisserman提出了堆叠多个3×3小卷积核，在中间使用更过非线性relu单元的方法来近似大卷积核（比如5$\times$5,7$\times$7），并且取得了良好的图像分类表现，此卷积网络的结构被称为VGG。各种各样的技术已经被开发以改善图像分类，包括批处理归一化，参数化RELU，空间金字塔池化。受到网络[22]中的网络概念的启发，不同的GoogleNet（又名Inception）模型被提出来增强在ImageNet上的表现。最近,He等人提出了深度残差网络(Resnets)，在2015年ImageNet挑战中赢得了多个赛道。通过使用残差链接，在训练深度网络时可以减少过拟合。

深度学习也被应用于视频理解。人们提出了用于识别视频中人类动作的三维卷积（Ji等），并在限制玻尔兹曼机和堆叠 ISA中使用了三维卷积来学习时空特征。Karpathy等人[14]提出了不同的融合方法用于视频分类。Simonyan和Zisserman[33]使用了双流网络来实现高精度的动作识别。Feichtenhofer等人使用Resnet架构和流之间的附加连接增强了这些双流网络。一些基于双流网络的方法包括Temporal Segment Networks[44]，Action Transformations和 Convolutional Fusion被提出并取得了人的动作识别的最好精度。最近，Tran等人提出在一个大型数据集上训练一种名为C3D的深度3D卷积网架构，用于时空特征学习。C3D特征在动作识别、动作检测、视频字幕、手势检测等任务上都有较强的性能。

本文的方法主要与C3D和Resnet有关。与C3D类似，我们使用3D ConvNets来学习时空特征。然而，在C3D中，虽然工作仅限于寻找三维卷积时间维度上的卷积核核长度，但我们考虑许多了结构设计的其他方面。此外，我们的search旨在比较不同的架构，同时限制模型容量(参数的数量)。我们的工作也与Resnet有关，我们将我们的搜索限制为Resnet架构。然而，我们强调Resnet在视频表示中的应用是具有挑战性的，因为我们需要考虑许多非琐碎的问题，我们通过精心设计的实验来经验地回答这些问题（在第三节）。我们新提出的架构(Res3D)相对于C3D在5种不同的基准测试中表现良好，运行时速度快了2倍，模型尺寸小了2倍。

### 3. 结构搜索

本节中，我们进行了大规模的卷积网络结构测试来实施时空特征学习，我们从C3D开始，因为它通常被用作视频的深度表示。由于残差网络表现良好且简单，我们将索索空间限制在残差网络中。由于训练深度网络的计算成本很高，我们在UCF101的一个子集上进行架构搜索。我们还注意到，这些网络的高内存消耗，加上计算批处理规范化统计数据需要大量的minibatch，阻碍了探索模型空间的某些部分。在设计我们最终提出的ConvNet架构时，采用了这些实验的观察结果。稍后，我们在Sports-1M上训练我们的最终架构，并展示了它在不同视频理解任务中学习时空特性的好处。

#### 3.1 对普遍模型的规定

这些都是基于一个小基准(UCF101)的实证结果。我们试图通过限制在搜索时每个网络的容量来限制过拟合的不好结果，因此，性能上的差异可以更多地归因于网络的设计，而不是它的大小。然而，有人可能会问，这些发现是否可以推广到其他数据集(特别是像Sports-1M这样的大型数据集)。虽然不能在大规模数据集上重复每个实验，但我们将在接下来的章节中选择几个模型进行实验来表明结果是一致的。我们强调，虽然这个协议并不理想，但它是实用的，并且由此产生的直觉是有价值的——然而，我们鼓励开发更适合架构搜索的基准（数据集）。


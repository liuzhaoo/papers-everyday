### 摘要

 提出了深度3D残差网络，性能比C3D网络好得多：比C3D快两倍且小两倍。表示更加紧凑

### 引言

改进ConvNet架构的设计已经刺激了图像理解方面的重大进展：从AlexNet到VGG，然后是GoogleNet，最后是ResNet。视频理解是计算机视觉的另一个基础的问题，但视频分类架构[14,24,33]和表示学习[41]的进展较慢。阻碍强大结构的因素主要有三个：第一，与图像模型相比，视频的计算量和内存消耗更高。比如在C3D模型中，在UCF101上训练模型需要3到4天，在Sports-1M上需要两个月，因此导致在UCF101上寻找广泛的结构很难。第二，目前还没有一个用于视频架构搜索的标准基准。在静态的图像中，可以在一个合理的时间内在ImageNet上训练网络，而且在ImageNet上表现良好的结构已经被证明可以用于其他任务，比如对象检测和分割。在视频领域，Sport-1M被证明在通用特征学习上是有用的，但是用它来进行寻找结构还是太大了。相比之下，虽然UCF101和ImageNet有相似的帧数，但是它们是高度相关的，并且设置被严格控制。因此，在这个基准上训练的模型很容易过拟合，[41,14]的实验表明，从头训练的ConvNets可以获得41 - 44%的精度，而在UCF101上对Sports1M惊醒微调可以将精度提高到82%。第三，设计视频分类模型并非易事，有许多选择产生的结果是敏感的，其中包括如何对输入进行采样和预处理、卷积的类型、使用多少层以及如何对时间维度建模。因此，虽然图像领域的进展显然应该纳入视频建模，单纯地将图像模型转换为视频分类(例如简单地将2D Resnet应用于视频帧)是次优的。

在本文中，我们通过在一个小型基准上(UCF101)寻找一个精心设计的架构搜索来解决这些问题，有人可能会说，这些发现的普遍性受到了数据集的偏差的限制，会将搜索的结果过拟合到UCF101上。我们通过两项努力来解决这个问题。首先，我们限制这些网络使它们有相似的容量（参数的数量），他们仍然会过拟合，但是精度的提高更多的归功于结构的单一变化而不是容量。其次，在这个小数据集架构的搜索上使我们得到一个有效的三维深度残差网络，当我们在更大的数据集上(sport - 1m)进行训练时，证明了它的有效性，而且在不同的视频基准测试中都取得了不俗的成绩。综上所述，本文的贡献如下:

- 通过在UCF101上进行行为识别的训练来进行跨多个维度的ConvNet架构搜索，并提出对每个维度敏感性的经验观察

- 我们提出(据我们所知)第一个深度3D残差网络并在大规模视频基准上训练，用于时空特征学习。
- 我们的时空特征在Sport-1M（不使用长时间模型），UCF101和HMDB51（只考虑RGB输入）上，以及在THUMOS14和ASLAN竞赛的表现上都取得了sota性能。
- 我们的模型比目前的深度视频特征快2倍，小2倍，更紧凑。

### 2. 相关工作

视频理解是计算机视觉的核心问题之一，已经研究了几十年。许多对视频理解的研究都集中在发展视频的时空特征上。一些提出的视频表征包括时空兴趣点STIPs，SIFT-3D，HOG3D，Cuboids和ActionBank。这些表示是手工设计的，使用不同的特征编码方案，如特征直方图或金字塔在手工制作的表示中，改进密集轨迹(iDT)被认为是目前最先进的手工制作特征，在不同的视频分类问题上有很好的结果。

自计算机视觉深度学习在2012年ImageNet挑战赛上亮相取得突破以来，许多基于卷积神经网络的方法被提出来用于图像识别。Simonyan和Zisserman提出了堆叠多个3×3小卷积核，在中间使用更过非线性relu单元的方法来近似大卷积核（比如5$\times$5,7$\times$7），并且取得了良好的图像分类表现，此卷积网络的结构被称为VGG。各种各样的技术已经被开发以改善图像分类，包括批处理归一化，参数化RELU，空间金字塔池化。受到网络[22]中的网络概念的启发，不同的GoogleNet（又名Inception）模型被提出来增强在ImageNet上的表现。最近,He等人提出了深度残差网络(Resnets)，在2015年ImageNet挑战中赢得了多个赛道。通过使用残差链接，在训练深度网络时可以减少过拟合。

深度学习也被应用于视频理解。人们提出了用于识别视频中人类动作的三维卷积（Ji等），并在限制玻尔兹曼机和堆叠 ISA中使用了三维卷积来学习时空特征。Karpathy等人[14]提出了不同的融合方法用于视频分类。Simonyan和Zisserman[33]使用了双流网络来实现高精度的动作识别。Feichtenhofer等人使用Resnet架构和流之间的附加连接增强了这些双流网络。一些基于双流网络的方法包括Temporal Segment Networks[44]，Action Transformations和 Convolutional Fusion被提出并取得了人的动作识别的最好精度。最近，Tran等人提出在一个大型数据集上训练一种名为C3D的深度3D卷积网架构，用于时空特征学习。C3D特征在动作识别、动作检测、视频字幕、手势检测等任务上都有较强的性能。

本文的方法主要与C3D和Resnet有关。与C3D类似，我们使用3D ConvNets来学习时空特征。然而，在C3D中，虽然工作仅限于寻找三维卷积时间维度上的卷积核核长度，但我们考虑许多了结构设计的其他方面。此外，我们的search旨在比较不同的架构，同时限制模型容量(参数的数量)。我们的工作也与Resnet有关，我们将我们的搜索限制为Resnet架构。然而，我们强调Resnet在视频表示中的应用是具有挑战性的，因为我们需要考虑许多非琐碎的问题，我们通过精心设计的实验来经验地回答这些问题（在第三节）。我们新提出的架构(Res3D)相对于C3D在5种不同的基准测试中表现良好，运行时速度快了2倍，模型尺寸小了2倍。

### 3. 结构搜索

本节中，我们进行了大规模的卷积网络结构测试来实施时空特征学习，我们从C3D开始，因为它通常被用作视频的深度表示。由于残差网络表现良好且简单，我们将索索空间限制在残差网络中。由于训练深度网络的计算成本很高，我们在UCF101的一个子集上进行架构搜索。我们还注意到，这些网络的高内存消耗，加上计算批处理规范化统计数据需要大量的minibatch，阻碍了探索模型空间的某些部分。在设计我们最终提出的ConvNet架构时，采用了这些实验的观察结果。稍后，我们在Sports-1M上训练我们的最终架构，并展示了它在不同视频理解任务中学习时空特性的好处。

#### 3.1 对普遍模型的规定

这些都是基于一个小基准(UCF101)的实证结果。我们试图通过限制在搜索时每个网络的容量来限制过拟合的不好结果，因此，性能上的差异可以更多地归因于网络的设计，而不是它的大小。然而，有人可能会问，这些发现是否可以推广到其他数据集(特别是像Sports-1M这样的大型数据集)。虽然不能在大规模数据集上重复每个实验，但我们将在接下来的章节中选择几个模型进行实验来表明结果是一致的。我们强调，虽然这个协议并不理想，但它是实用的，并且由此产生的直觉是有价值的——然而，我们鼓励开发更适合架构搜索的基准（数据集）。

### 3.2 3D残差网络

**note**：为了简便，我们忽略了通道，并且将输入，卷积核，和输出表示为L$\times$H$\times$W 的 3D tensor   L,H,W分别代表时间长度，高度和宽度

基础架构：基本的3D残差结构如图2所示

<img src="https://raw.githubusercontent.com/liuzhaoo/markdown_pics/master/img/3D RS B.png" style="zoom:67%;" />

这些网络使用 8$\times$112$\times$112 的输入，这是可以适应GPU的最大内存限制和保持足够大的mini-batch的输入大小。然而，我们每隔一帧就跳过一帧，这相当于使用C3D的输入并且去掉偶数帧。总之，我们对2D残差网络做了这样的修改：将输入由224$\times$224 变为8$\times$112$\times$112 ,将所有的卷积层从$d\times d$ 变为$3\times d\times d$  ,所有的下采样层（每个大的layer的第一个block）使用的stride为$2\times2\times2$，而第一个卷积层使用的stride为$1\times2\times2$ ，同时，去掉了第一个最大池化层

**训练与验证** 在UCF101的split1上使用batchsize为20 的SGD来训练这些网络 。跟C3D相似，视频的帧倍缩放为128x171 并且随机裁剪为$112\times 112$ 在所有卷积层中使用BN。学习率设置为0.01且在20k次迭代后处以10。训练在90k次迭代后停止（大概15epochs）。我们使用裁剪成$112\times112$ 的单个帧来代替输入，并且用2D来代替所有的3D操作来执行2D基线。表3列出了在UCF101上不同网络的clip测试精度。与2D参考模型相比，3DResnets表现更好，而更深层次的网络(34层)与2D或3D的18层网络相比，收效甚微。

<img src="https://raw.githubusercontent.com/liuzhaoo/markdown_pics/master/img/acc-on-ucf.png" style="zoom:80%;" />



我们注意到，由于这些网络的参数数量不同这些结果不能作为最终的结论，在之后的结构探索实验中，所有进行比较的模型都有大概33M个参数。

**简化的网络** 我们注意到，通过减小输入大小，我们可以进一步降低网络的复杂性和网络训练的内存消耗，从而快架构搜索的速度。输入为较小4×112×112的的情况下，需要将conv5_1的stride调整为1×2×2（时间维度上不再减半）。这种简化方法使3D-Resnet18的复杂度从19.3b降低到了10.3b运算同时保持了在UCF101上的精度。从现在起，我们将这个网络体系结构表示为SR18(简化的3D-Resnet18)，并使用它作为基线来修改我们接下来的架构搜索实验。

**观察1** 在UCF101上使用4帧的输入和一个深度18的网络(SR18)实现了良好的基线性能和快速训练。

#### 3.3 最佳帧采样频率

我们使用SR18并且使用集合{1，2，4，8，16，32}中的数字作为输入帧的时间步长。在最低的情况下，输入是连续的4个帧，大约是1/8秒长的切片。若使用步长32，输入切片是从一个128帧长的切片上的粗略采样（大约4.5-5秒）。表4为SR18使用不同采样率的输入进行训练的精度。

<img src="https://raw.githubusercontent.com/liuzhaoo/markdown_pics/master/img/sample_rate.png" style="zoom:80%;" />

**观察2 ** 对于视频分类，从2-4帧(对于25-30fps内的视频)中采样一帧，并且使用0.25s到0.75s的切片长度可以获得良好的精度。

#### 3.4 最佳输入分辨率

在[41]中，Tran等人进行了一项研究输入分辨率的实验。但由于它们的网络参数数目不同，因此可以预测，会产生不同程度的过拟合。我们进行了一个类似的实验来确定一个好的视频分类输入分辨率，但是再次限制我们的网络使用相似数量的参数。实验选用3种不同的输入分辨率，分别为224×224、112×112、56×56，对应的缩放帧大小分别为256×342、128×171、64×86。我们调整了这些网络的conv1层的卷积核大小，使它们具有相似的感受野，对于224、112、56的输入，分别使用3×11×11, 3×7×7, 3×5×5的卷积核，对应的stride为 1×4×4，1×2×2、1×1×1。这三种网络的其余部分保持不变，因此参数上的唯一区别来自于conv1层，它与参数总数相比很小。

<img src="https://raw.githubusercontent.com/liuzhaoo/markdown_pics/master/img/res.png" style="zoom:80%;" />

**观察3** 输入分辨率为128 (crop 112)对于计算复杂度和视频分类的精度来说都是理想的，因为GPU内存的限制。

#### 3.5 使用哪种类型的卷积

关于用于视频分类的卷积类型有很多猜测。人们可以选择使用二维卷积网络，如双流网络。全三卷积比如C3D。甚至是一个混合了2D和3D操作的卷积网络。在本节中，我们将混合3D-2D卷积网、混合2D-1D卷积网与全3d卷积网(SR18)进行比较，以解决这些猜想。



。。。



**观察4** 在所有层上使用3D卷积似乎可以提高视频分类性能。

#### 3.6 你的网络够深吗

在这里，我们使用SR18作为参考模型，并使用相同的参数数量限制来改变网络深度。我们再次设计了网络宽度k，使第1组和第2组的所有卷积层(conv1和conv2 x)都有k个输出通道。使3，4，5的卷积层有2k，4k，8k个输出通道。通过改变每组的块数，我们得到了不同深度的网络，并再次调整k来匹配参数的数量。

**观察5**网络深度为18层，可以很好地平衡视频分类的准确性、计算复杂度和存储空间。

### 4. 使用3D Resnets进行时空特征学习

我们现在将第3节的观察结果应用于设计用于在大规模数据集(Sports-1M)上进行时空特征学习的网络。在第5节中，我们将学习到的特征与当前C3D特征在各种视频任务上进行比较

**结构** 与SR18相比，我们使用了8×112×112帧的输入，因为大规模训练可以从额外的信息中获益。其他观测均采用:时间步长为2，输入分辨率为112×112，全三维卷积，深度18。从现在开始，我们将这个体系结构表示为Res3D。

**训练** 类似于C3D，我们在Sports-1M上训练我们的Res3D来学习时空特征。这是一个大型数据集，大约有110万个视频，487种运动类别，并包括公共训练和测试子集。我们从每个训练视频中随机抽取5个2秒长的片段。将片段的分辨率调整为128×171。采用随机时空裁剪作为数据增强，将输入片段随机裁剪为8×112×112 (sampling stride 2)。训练在两块GPU上使用SGD，minibatch size为40。初始学习率为0.01，每250k次迭代除以2，在3M次迭代时结束。我们也用同样的程序训练了一个2D-Resnet18基线。

**在Sports1M上的结果** 表8给出了我们的Res3D的分类结果与现有方法的比较

<img src="https://raw.githubusercontent.com/liuzhaoo/markdown_pics/master/img/bijiao.png" style="zoom:80%;" />

对于top-1片段的精度，我们使用一个单一的中心裁剪剪辑和一个单一的模型。对于前1和前5的视频精度，我们使用10个中心裁剪和平均他们的预测来做出一个视频级别的预测。与不使用长时模型的单一模型相比，Res3D实现了最先进的性能。在前1名的片段、前1名的视频和前5名的视频准确率上，它的表现分别C3D[41]高出2.7%、4.5%和2.6%。与2D ConvNets相比，Res3D在top1视频准确率上比AlexNet和GoogleNet提高了2%和0.7%.我们注意到这些方法使用了240个片段，而Res3D只使用10个。在这个大规模基准上与随机机会相比，这些改进是显著的。与使用长期建模的方法相比，如LSTM或卷积池[24]，我们的Res3D比一个接受过AlexNet fc功能训练的LSTM好2.9%和4.2%，可与一个LSTM训练的GoogleNet fc功能媲美。精度较高的唯一方法是使用卷积池进行长时间建模。我们注意到，我们的方法不涉及任何长期建模，因为我们的主要目标是学习原子时空特征。***实际上，我们的Res3D模型可以采用LSTM或卷积池长期建模的正交方向，进一步提高运动分类精度。***

 图2显示了Res3D和2D-Resnet基线的已学习的conv1过滤器。这两个网络都是在Sports-1M上进行训练的;唯一的区别是3D的卷积被2D的卷积所取代。我们观察到:1)所有3D过滤器（卷积核）在时间维度上发生变化，这意味着每个过滤器都编码了时空信息(而不仅仅是空间信息); 2)对于大多数2D卷积核，我们可以找到一个具有类似外观模式的3D卷积核(大部分在内核的中心)。这可能表明，3D卷积核能够覆盖2D卷积核中的外观信息，但也可以捕获有用的运动信息。这证实了我们的发现(与[41]一致)，即3D卷积可以同时捕获外观和运动，因此非常适合用于时空特征学习。

**模型大小和复杂度** Res3D比C3D小两倍且比它快两倍Res3D有3320万个参数和193亿次运算，而C3D有7290万个参数和385亿次运算。

### 5 Res3D作为时空特征

在本节中，我们评估在Sports-1M上预先训练的Res3D模型作为视频理解任务的时空特征提取器，并与其他表征进行比较。

#### 5.1 行为识别

**数据集** 使用UCF101和HMDB51。两者都有3个训练/测试分割，所以我们用3倍交叉验证进行评估。

**模型**  我们将我们的Res3D与在sport-1M训练的C3D和2D-Resnet基线进行比较。我们使用作者建议的C3D的fc6激活层。对于Res3D和2D-Resnet，我们尝试了res5b和pool5特征。res5b的特性在2D和3D情况下都比pool5稍好，但差距很小;这里我们报告使用res5b的结果。我们通过提取片段特征并对其进行平均pooling，然后对得到的向量进行L2-normalizing来表示视频。采用线性支持向量机对动作进行分类。我们还对UCF101和HMDB51上的C3D和Res3D进行了微调，发现微调后的模型的性能差距更大，这表明更强大的架构从微调中获益更多。

表9显示了Res3D在UCF101和HMDB51上与C3D[41]和2D-Resnet基线相比的人类动作识别结果。我们的Res3D在UCF101和HMDB51上的表现分别比C3D好3.5%和3.3%。它达到了仅使用RGB输入的方法的最佳精度(见表10）。当仅使用RGB时，时序网络(TSNs)在UCF101上也能达到85.7%的高精度。使用两种模式(如RGB和RGB差分)时，TSNs甚至可以达到87.3%的高精度。我们注意到这些数字仅在split 1上求值，因此不能直接与我们的结果相比较。虽然在这里不可能进行直接的比较，但我们仍然可以推测TSNs和Res3D架构在相同的RGB模式下是相同的。在HMDB51上，Res3D在仅使用RGB输入的方法中性能最好。目前最先进的模型[44,5,4]使用昂贵的光流或iDT[45]增强RGB输入，但值得注意的是，这种增强的高计算成本阻碍了它们在大规模数据集上应用(例如Sports-1M)。最近的工作，I3D(与本工作同时进行)，在UCF101(98%)和HMDB51(80%)上取得了很好的性能，其双流I3D模型使用RGB和光流输入，以及使用Imagenet预训练模型。仅使用RGB时，他们在UCF101和HMDB51上的结果分别是84.5%和49.8%，比我们差1.3%和5.1%。我们注意到这些结果并不能直接进行比较，因为它们的数量只在UCF101和HMDB51的split 1上。


- Lin 等人提出网络中的网络，来提高神经网络的表达能力。如果应用到卷积层中，这种方法就可以被看作是再额外的1$\times$1 卷积层后跟着一个Relu函数。
- 1$\times$1 卷积的作用
  - 1. 降维，在计算比较大的卷积层之前，用它来降低输入特征图的通道数目以此来消除计算瓶颈（否则会限制我们网络的规模），这可以在不影响性能的情况下增加网络的深度和宽度。
  - 2. 增加网络层数
- 提高深度网络的性能的最直接的方法是增加size，包括深度和宽度。但是这样会产生很多的参数，从而使网络容易发生过拟合，尤其是训练样本有限的情况下。并且对算力的要求很大
  - 解决此问题的基本方法就是将全连接层转换为稀疏连接的结构
- 关于稀疏矩阵计算的大量文献表明，将稀疏矩阵聚类为相对密集的子矩阵，往往能使稀疏矩阵乘法的性能（速度）达到最好
- inception模块是一个一个堆叠的，所以它们的输出相关统计必然会发生变化: 高层模块会捕获更高级的抽象特性，它们（模块）的空间集中度会降低，因此在高层模块里会有更多的3$\times$3 和 5$\times$5 卷积。
  - 此模块的一个很大的问题是在包含大量卷积核（过滤器）的卷积层的顶部即使只有有限数量的 5$\times$5 卷积也可能十分耗费计算资源，尤其是加入池化层之后。
  - 只在较高的层开始使用Inception模块，同时保持较低的层以传统的卷积方式，这似乎是有益的
  - 这种架构的主要好处之一是，它允许在每个阶段显著增加单元的数量，而不会导致计算复杂度过度增大



- 结构 

<div></div>

<div align='center'> <img src="https://pic2.zhimg.com/80/v2-163bd498199e2862b94640a8883c607b_720w.jpg" style="zoom:80%;" /></div>

